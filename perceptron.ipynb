{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron algorithm is a type of linear classification algorithm used to classify data into two categories. It is a simple algorithm that learns from the mistakes made during the classification process and adjusts the weights of the input features to improve the accuracy of the classification. \n",
    "\n",
    "```python \n",
    "y_pred = sign(w0 + w1*x1 + w2*x2 + ... + wn*xn)\n",
    "wi = wi + learning_rate * (target - y_pred) * xi\n",
    "```\n",
    "\n",
    "Here is an implementation of the perceptron algorithm in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, lr=0.01, n_iter=100):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.weights = np.zeros(1 + X.shape[1])\n",
    "        self.errors = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.lr * (target - self.predict(xi))\n",
    "                self.weights[1:] += update * xi\n",
    "                self.weights[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors.append(errors)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.weights[1:]) + self.weights[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron class has the following methods:\n",
    "\n",
    "__init__(self, lr=0.01, n_iter=100): Initializes the perceptron with a learning rate (lr) and number of iterations (n_iter) to perform during training.\n",
    "\n",
    "fit(self, X, y): Trains the perceptron on the input data X and target labels y. The method initializes the weights to zero and iterates through the data n_iter times, adjusting the weights after each misclassification. The method returns the trained perceptron.\n",
    "\n",
    "net_input(self, X): Computes the weighted sum of inputs and bias.\n",
    "\n",
    "predict(self, X): Predicts the class label for a given input X based on the current weights.\n",
    "\n",
    "To use the perceptron algorithm, you can create an instance of the Perceptron class, and then call the fit method with your input data X and target labels y. Here is an example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[2.0, 1.0], [3.0, 4.0], [4.0, 2.0], [3.0, 1.0]])\n",
    "y = np.array([-1, 1, 1, -1])\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "new_X = np.array([[5.0, 2.0], [1.0, 3.0]])\n",
    "perceptron.predict(new_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3764518162273597\n",
      "Epoch 1000, Loss: 0.24965358535236715\n",
      "Epoch 2000, Loss: 0.2490915145023216\n",
      "Epoch 3000, Loss: 0.24868501067882615\n",
      "Epoch 4000, Loss: 0.2485159855419558\n",
      "Epoch 5000, Loss: 0.24860723024344678\n",
      "Epoch 6000, Loss: 0.24887270591470728\n",
      "Epoch 7000, Loss: 0.24917823508506215\n",
      "Epoch 8000, Loss: 0.24942796667025774\n",
      "Epoch 9000, Loss: 0.24958802454526538\n",
      "Predictions:\n",
      "[[0.48096412]\n",
      " [0.50103142]\n",
      " [0.50091049]\n",
      " [0.51890476]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the neural network parameters and hyperparameters.\n",
    "        \n",
    "        Args:\n",
    "        - input_size (int): Number of input features.\n",
    "        - hidden_size (int): Number of neurons in the hidden layer.\n",
    "        - output_size (int): Number of output neurons.\n",
    "        - learning_rate (float): Learning rate for weight updates.\n",
    "        \"\"\"\n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "        self.bias_hidden = np.random.rand(hidden_size)\n",
    "        self.bias_output = np.random.rand(output_size)\n",
    "        \n",
    "        # Set learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Compute the sigmoid activation function.\n",
    "        \n",
    "        Args:\n",
    "        - x (np.ndarray): Input values.\n",
    "        \n",
    "        Returns:\n",
    "        - np.ndarray: Sigmoid of the input.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the sigmoid function.\n",
    "        \n",
    "        Args:\n",
    "        - x (np.ndarray): Input values.\n",
    "        \n",
    "        Returns:\n",
    "        - np.ndarray: Derivative of the sigmoid function at the input.\n",
    "        \"\"\"\n",
    "        sig = self.sigmoid(x)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "        - X (np.ndarray): Input data.\n",
    "        \n",
    "        Returns:\n",
    "        - np.ndarray: Output of the network.\n",
    "        \"\"\"\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.y_pred = self.sigmoid(self.final_input)\n",
    "        return self.y_pred\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform a backward pass to update weights and biases.\n",
    "        \n",
    "        Args:\n",
    "        - X (np.ndarray): Input data.\n",
    "        - y (np.ndarray): True labels.\n",
    "        \"\"\"\n",
    "        # Compute loss\n",
    "        loss = np.mean((y - self.y_pred) ** 2)\n",
    "\n",
    "        # Compute gradients\n",
    "        d_loss_y_pred = self.y_pred - y\n",
    "        d_y_pred_final_input = self.sigmoid_derivative(self.final_input)\n",
    "        d_final_input_weights_hidden_output = self.hidden_output\n",
    "\n",
    "        d_loss_final_input = d_loss_y_pred * d_y_pred_final_input\n",
    "        d_loss_weights_hidden_output = np.dot(d_final_input_weights_hidden_output.T, d_loss_final_input)\n",
    "        d_loss_bias_output = np.sum(d_loss_final_input, axis=0)\n",
    "\n",
    "        d_hidden_output_hidden_input = self.sigmoid_derivative(self.hidden_output)\n",
    "        d_hidden_input_weights_input_hidden = X\n",
    "\n",
    "        d_loss_hidden_output = np.dot(d_loss_final_input, self.weights_hidden_output.T)\n",
    "        d_loss_hidden_input = d_loss_hidden_output * d_hidden_output_hidden_input\n",
    "        d_loss_weights_input_hidden = np.dot(d_hidden_input_weights_input_hidden.T, d_loss_hidden_input)\n",
    "        d_loss_bias_hidden = np.sum(d_loss_hidden_input, axis=0)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output -= self.learning_rate * d_loss_weights_hidden_output\n",
    "        self.bias_output -= self.learning_rate * d_loss_bias_output\n",
    "        self.weights_input_hidden -= self.learning_rate * d_loss_weights_input_hidden\n",
    "        self.bias_hidden -= self.learning_rate * d_loss_bias_hidden\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self, X, y, epochs=10000):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        Args:\n",
    "        - X (np.ndarray): Input data.\n",
    "        - y (np.ndarray): True labels.\n",
    "        - epochs (int): Number of training epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            self.forward(X)\n",
    "            # Backward pass\n",
    "            loss = self.backward(X, y)\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model.\n",
    "        \n",
    "        Args:\n",
    "        - X (np.ndarray): Input data.\n",
    "        \n",
    "        Returns:\n",
    "        - np.ndarray: Predicted values.\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the neural network\n",
    "    input_size = 2\n",
    "    hidden_size = 2\n",
    "    output_size = 1\n",
    "    learning_rate = 0.1\n",
    "    nn = SimpleNeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
    "\n",
    "    # Training data for XOR problem\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "    # Train the network\n",
    "    nn.train(X, y, epochs=10000)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = nn.predict(X)\n",
    "    print(\"Predictions:\")\n",
    "    print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=SimpleNeuralNetwork(5,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57019677],\n",
       "       [0.43860151],\n",
       "       [0.98837384],\n",
       "       [0.10204481],\n",
       "       [0.20887676],\n",
       "       [0.16130952],\n",
       "       [0.65310833],\n",
       "       [0.2532916 ],\n",
       "       [0.46631077],\n",
       "       [0.24442559]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.weights_hidden_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.weights_input_hidden.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4686512 , 0.97676109, 0.60484552, 0.73926358, 0.03918779],\n",
       "       [0.28280696, 0.12019656, 0.2961402 , 0.11872772, 0.31798318],\n",
       "       [0.41426299, 0.0641475 , 0.69247212, 0.56660145, 0.26538949]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97645947])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.bias_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, input_dim, hid_dim, output_dim, lr=0.001):\n",
    "        self.input_hidden_weights = np.random.randn(input_dim, hid_dim)\n",
    "        self.hidden_output_weights = np.random.randn(hid_dim, output_dim)\n",
    "        self.hidden_bias = np.random.rand(hid_dim)\n",
    "        self.output_bias = np.random.rand(output_dim)\n",
    "        self.lr = lr\n",
    "     \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def derivative_sigmoid(self, x):\n",
    "        sig = self.sigmoid(x)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.hidden_input = np.dot(X, self.input_hidden_weights) + self.hidden_bias\n",
    "        self.hidden_output = np.dot(self.hidden_input, self.hidden_output_weights) + self.output_bias\n",
    "        self.output_final = self.sigmoid(self.hidden_output)\n",
    "        self.y_val = np.round(self.output_final, 3)\n",
    "        return self.y_val\n",
    "    \n",
    "    def train(self, X, y, epochs):\n",
    "        for _ in range(epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, y)\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[0]  # Number of data points\n",
    "\n",
    "        # Calculate loss\n",
    "        self.loss = np.mean((y - self.y_val) ** 2)\n",
    "        \n",
    "        # Output layer error\n",
    "        dl_dy_val = self.y_val - y\n",
    "        dl_dz_output = dl_dy_val * self.derivative_sigmoid(self.hidden_output)\n",
    "        \n",
    "        # Gradients for hidden-output weights and biases\n",
    "        dl_hidden_output_weights = np.dot(self.hidden_input.T, dl_dz_output) / m\n",
    "        dl_output_bias = np.sum(dl_dz_output, axis=0) / m\n",
    "        \n",
    "        # Hidden layer error\n",
    "        dl_hidden_output = np.dot(dl_dz_output, self.hidden_output_weights.T)\n",
    "        \n",
    "        # Gradients for input-hidden weights and biases\n",
    "        dl_input_hidden_weights = np.dot(X.T, dl_hidden_output) / m\n",
    "        dl_hidden_bias = np.sum(dl_hidden_output, axis=0) / m\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.hidden_output_weights -= self.lr * dl_hidden_output_weights\n",
    "        self.output_bias -= self.lr * dl_output_bias\n",
    "        self.input_hidden_weights -= self.lr * dl_input_hidden_weights\n",
    "        self.hidden_bias -= self.lr * dl_hidden_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=NeuralNet(4,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[1,2,3,4],[1,4,2,1]],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.input_hidden_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.input_hidden_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.687],\n",
       "       [0.532]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.train(X,np.array([[1],[4]]),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6901636 ],\n",
       "       [0.53816513]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.output_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1=np.random.randn(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[1,2,3,4],[1,4,2,1]],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21869238, -4.15386666,  8.59065342,  0.11165262,  7.52637846],\n",
       "       [-0.53758915,  0.47764507,  5.96139666, -0.91113334,  4.64987846]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X,k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=np.array([[1,2,3],\n",
    "             [4,5,6],\n",
    "             [7,8,9],\n",
    "             [10,11,12]\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 32, 50, 68])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x1,w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([70, 80, 90])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(w1,x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
